{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy-dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment\n",
      "\n",
      "\n",
      "a [-0.00222291  0.00859151] [ 1.  0.  0.  0.  0.  0.]\n",
      "b [ 0.00957477 -0.01385114] [ 0.  1.  0.  0.  0.  0.]\n",
      "c [ 0.01293331  0.00684991] [ 0.  0.  1.  0.  0.  0.]\n",
      "d [-0.00456683  0.00329325] [ 0.  0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Codewords a , b, c, d, e, f\n",
    "\n",
    "# forms (3-5) A (1-2) B (3-5) C\n",
    "\n",
    "# specific forms abc, dbe, afc, afe, cbd, eba, cfd, efa\n",
    "\n",
    "# z dimension : 8 with variance around parameters\n",
    "\n",
    "# Target result: feature embeddings into 2 dimensions produce mappings b ~= f, a ~= d, c ~= e\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.insert(0,'/scratch/Jack/projects/Chamber')\n",
    "sys.path.insert(0,'/scratch/Jack/projects/Explanations')\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from models import VAE, ConvVAE, W2V, Analogy\n",
    "from chamber import Chamber, Misc, Historian, Oracle, Commander\n",
    "\n",
    "def generate_sequence(n=1):\n",
    "    p = []\n",
    "    for i in range(n):\n",
    "        random_pairs = [('a','b'),('b','b'),('c','b'),('a','d'),('c','d'),('d','d')]\n",
    "        # a and b are equivalent, but different from c and d\n",
    "        pi = random.choice(random_pairs)\n",
    "        for pi in random.choice(random_pairs):\n",
    "            p += [generate_latent(pi) for x in range(random.randint(1,1))]\n",
    "        #p += [generate_latent(p1) for x in range(random.randint(1,1))]+[generate_latent(p2) for x in range(random.randint(1,1))]+[generate_latent(p3) for x in range(random.randint(1,1))] \n",
    "    \n",
    "    return p\n",
    "\n",
    "def generate_latent(c, one_hot = True):\n",
    "    if one_hot:\n",
    "        t = torch.Tensor([0 for i in range(6)])\n",
    "        t[ord(c)-ord('a')] = 1\n",
    "        return [t]+[ord(c)-ord('a')]\n",
    "    else:\n",
    "        if c == 'a':\n",
    "            return [torch.Tensor([1.0, 1.0])]+[ord(c)-ord('a')]\n",
    "        if c == 'b':\n",
    "            return [torch.Tensor([0.0, 0.0])]+[ord(c)-ord('a')]\n",
    "        if c == 'c':\n",
    "            return [torch.Tensor([1.0, 0.0])]+[ord(c)-ord('a')]\n",
    "        if c == 'd':\n",
    "            return [torch.Tensor([0.0 ,1.0])]+[ord(c)-ord('a')]\n",
    "        \n",
    "epochs = 5000\n",
    "near_window = 1\n",
    "back_window = False\n",
    "\n",
    "\n",
    "m = torch.nn.LogSoftmax()\n",
    "loss = torch.nn.NLLLoss().cuda()\n",
    "optimizer = torch.optim.Adam(w2v.parameters(), weight_decay=0, lr=0.01)\n",
    "\n",
    "# So the training over latent space doesn't work as well as it should\n",
    "# Now we use the analogy model instead on train_analogy\n",
    "def train(word_pair):\n",
    "    in_word = Variable(word_pair[0][0].cuda()).unsqueeze(0)\n",
    "    out_word = Variable(torch.LongTensor([word_pair[1][1]]).cuda()).long()\n",
    "    \n",
    "    # categorical out_word\n",
    "    out_pred = w2v(in_word)\n",
    "    \n",
    "    \n",
    "    myloss = loss(m(out_pred[1]), out_word)\n",
    "    n = m(out_pred[1]).exp().data.cpu().numpy()[0]\n",
    "    #print(myloss.data.cpu().numpy()[0])\n",
    "    #for el in n:\n",
    "    #    print(\"{:0.2f}\".format(float(el)),end=\" , \"), np.around(n,decimals=2)\n",
    "    #print(\"\")\n",
    "    #print(out_word.data.cpu().numpy()[0], in_word.data.cpu().numpy()[0], out_pred[0].data.cpu().numpy()[0])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    myloss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "loss_analogy = torch.nn.MSELoss().cuda()\n",
    "dist = torch.nn.PairwiseDistance().cuda()\n",
    "optimizer_analogy = torch.optim.Adam(analogy.parameters(), weight_decay=0, lr=0.001)\n",
    "def train_analogy(word_pair):\n",
    "    in_word = Variable(word_pair[0][0].cuda()).unsqueeze(0)\n",
    "    out_word = Variable(word_pair[1][0].cuda()).unsqueeze(0)\n",
    "    \n",
    "    in_embedding = analogy.embed(in_word)\n",
    "    out_embedding = analogy.embed(out_word)\n",
    "    # categorical out_word\n",
    "    out_pred = analogy(in_embedding)\n",
    "    \n",
    "    distance = dist(out_pred[1], out_embedding)\n",
    "    myloss = loss_analogy(distance, Variable(torch.Tensor([0.0]).cuda()))\n",
    "    \n",
    "    #print(myloss.data.cpu().numpy()[0])\n",
    "    \n",
    "    optimizer_analogy.zero_grad()\n",
    "    myloss.backward()\n",
    "    optimizer_analogy.step()\n",
    "    \n",
    "experiments = 1\n",
    "\n",
    "w2v = W2V(dict_size=6, hidden_features=2).cuda()\n",
    "analogy = Analogy().cuda()\n",
    "\n",
    "for experiment in range(experiments):\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        s = generate_sequence()\n",
    "        for word in range(len(s)):\n",
    "            for l in range(-near_window,near_window+1):\n",
    "                if l == 0:\n",
    "                    continue\n",
    "\n",
    "                if l < 0 and back_window == False:\n",
    "                    continue\n",
    "\n",
    "                word_near = word-l\n",
    "                if word_near < 0 or word_near >= len(s):\n",
    "                    continue\n",
    "                train([s[word], s[word_near]])\n",
    "                #print(s[word][1], s[word_near][1])\n",
    "                #train_analogy([s[word], s[word_near]])\n",
    "\n",
    "    print(\"\\nExperiment\\n\\n\")\n",
    "    for var in ['a','b','c','d']:\n",
    "        in_word = Variable(generate_latent(var)[0].cuda()).unsqueeze(0)\n",
    "        embed = w2v(in_word)\n",
    "        print(var, embed[0].data.cpu().numpy()[0], in_word.data.cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
